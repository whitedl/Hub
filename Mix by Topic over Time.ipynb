{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries:\n",
    "By: Patrick Warren (patrick.lee.warren@gmail.com)\n",
    "\n",
    "Sections\n",
    "1. Preliminaries\n",
    "    1. Load packages and set some parameters\n",
    "    2. Define topic and dates\n",
    "2. Hydrate Tweets and Gather Them Together in a Topic (Only Need to Do This Once)\n",
    "3. Define Account/Tweet Labels of Interest\n",
    "    1. Transform account/tweet stats into labels\n",
    "4. Pre-Specified Conversation Approach\n",
    "    1. In the cross-section\n",
    "    2. Over time\n",
    "5. Hashtag-Based Unsupervised Approach\n",
    "    1. Gather top hashtags\n",
    "    2. Weekly outliers in type share\n",
    "    3. Daily outliers in type share\n",
    "6. NLP Unsupervised Approach\n",
    "    1. Narrow dataset for NLP-- Time and language\n",
    "    2. Clean up, stem, and vectorize (TF-IDF weighted bag of words)\n",
    "    3. Apply k-means clustering \n",
    "    4. Apply LDA classification\n",
    "    5. Visualize convo as \"discovered\" in k-mean/LDA\n",
    "7. Hunt Graph Creation\n",
    "    1. Specify targets by terms and time\n",
    "    2. Gather data and build visualization of last 200 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A Load Packages; Set some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http\n",
    "from http import client\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import sleep\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 24\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "#plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(s):\n",
    "    return [i  for i in s.split() if i.startswith(\"#\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specific credentials for our Twitter API Access. \n",
    "# This flow will as a function of access/capabilities\n",
    "####\n",
    "\n",
    "import pickle\n",
    "import tweepy\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/home/pwarren/local_trolls/secret_twitter_credentials.pkl'):\n",
    "    Twitter={}\n",
    "    Twitter['Consumer Key'] = ''\n",
    "    Twitter['Consumer Secret'] = ''\n",
    "    Twitter['Access Token'] = ''\n",
    "    Twitter['Access Token Secret'] = ''\n",
    "    with open('/home/pwarren/local_trolls/secret_twitter_credentials.pkl','wb') as f:\n",
    "        pickle.dump(Twitter, f)\n",
    "else:\n",
    "    Twitter=pickle.load(open('/home/pwarren/local_trolls/secret_twitter_credentials.pkl','rb'))\n",
    "\n",
    "\n",
    "## Authorizing an application to access Twitter account data\n",
    "\n",
    "auth = tweepy.OAuthHandler(Twitter['Consumer Key'], Twitter['Consumer Secret'])\n",
    "auth.set_access_token(Twitter['Access Token'], Twitter['Access Token Secret'])\n",
    "\n",
    "\n",
    "twitter_api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B Define Topic Stem and Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem is short-hand for topic. Often, it is the word that we use to cast the wide net. If the topic definition is wider,\n",
    "# it's short-hand for that. It is also the stem in the file structure, that we use to \"sum up\" over dates from the raw \n",
    "# data files.\n",
    "#\n",
    "# In general, our raw data files are of the form [stem]_[date].csv, and are exports from the Social Studio\n",
    "# platform. \n",
    "####\n",
    "\n",
    "stem='debates_keyhashtags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### When the topic search includes several files, each which includes data for one or more dates, this variable captures a list \n",
    "# of the dates use to aggregate the individual files\n",
    "\n",
    "dates=['0901_0922']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hydrate SS Tweets on  Topic by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch2/pwarren/hunt/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Bring in the individual SS files, hydrate them with Twitter API (or appropriate), and save the hydrated stem/date .jsons\n",
    "# \n",
    "#########\n",
    "\n",
    "running_tweets=pd.DataFrame()\n",
    "\n",
    "for date in dates:\n",
    "    file_encoding = 'utf-8'        # set file_encoding to the file encoding (utf8, latin1, etc.)\n",
    "    input_fd = open(stem+'_'+date+'.csv', encoding=file_encoding, errors = 'backslashreplace')\n",
    "    social_studio_tweets=pd.read_csv(input_fd, sep=',', dtype=str)\n",
    "\n",
    "    social_studio_tweets_tomatch=social_studio_tweets[social_studio_tweets['EXTERNAL_ID'].notnull()]['EXTERNAL_ID'].drop_duplicates()\n",
    "\n",
    "    test=social_studio_tweets_tomatch[0:100].tolist()\n",
    "    api_tweets=twitter_api.statuses_lookup(test)\n",
    "\n",
    "    for i in range(100, len(social_studio_tweets_tomatch), 100):\n",
    "        test =social_studio_tweets_tomatch[i:i+100].tolist()\n",
    "        try: api_tweets.extend(twitter_api.statuses_lookup(test))\n",
    "        except tweepy.TweepError:\n",
    "            print ('TweepError')\n",
    "            sleep(30)\n",
    "        \n",
    "    json_data = [r._json for r in api_tweets]\n",
    "    api_tweets_pd = pd.json_normalize(json_data)\n",
    "\n",
    "    social_studio_tweets.rename(columns={'EXTERNAL_ID':'id_str'}, inplace=True)\n",
    "    full_tweets=social_studio_tweets.merge(api_tweets_pd, how='left', on='id_str')\n",
    "    full_tweets.to_json(stem+'_'+date+'_full.json')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Hydrated Data by Data, narrow to analysis columns, join narrowed data, and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch2/pwarren/hunt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API results are quite wide and we are only interested in a handful of the fields. This list defines that set of fields.\n",
    "\n",
    "narrow_cols=['created_at','id_str','in_reply_to_status_id','lang','POST_TYPE','retweeted_status.id','retweeted_status.created_at','retweeted_status.user.created_at','retweeted_status.user.statuses_count','retweeted_status.user.verified','source','text','user.created_at','user.default_profile_image','user.description','user.favourites_count','user.followers_count','user.friends_count','user.geo_enabled','user.id_str','user.listed_count','user.location','user.name','user.screen_name','user.statuses_count','user.verified']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring in the hydrated data for each date, limit to narrow columns, and append them to a master narrow topic DataFrame \n",
    "\n",
    "topic_narrow=pd.DataFrame()\n",
    "for date in dates:\n",
    "    topic=pd.read_json(stem+'_'+date+'_full.json')\n",
    "    topic_narrow=topic_narrow.append(topic[narrow_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_narrow.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_narrow.shape)\n",
    "topic_narrow.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export master narrow topic dataframe as .json, to be stored for future analysis\n",
    "\n",
    "topic_narrow.to_json(stem+'_narrow.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_narrow.to_csv(stem+'_narrow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define and Calculate Account/Tweet Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring in the Narrow Data\n",
    "topic_narrow=pd.read_json(stem+'_narrow.json', convert_dates=['user.created_at','created_at'],dtype={'id_str':'str','user.id_str':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_narrow['source'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A Transform  account/tweet metrics into labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator for API getting tweet text, since Social Studio often has records, without full text of deleted tweets\n",
    "fulltext_narrow=(topic_narrow['text'].notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_narrow['user.created_at'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_narrow['birth_date']=pd.to_datetime(topic_narrow['user.created_at'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate various derivative tweet and accounts statistics\n",
    "\n",
    "topic_narrow['date_time']=pd.to_datetime(topic_narrow['created_at'], format=\"%Y-%b-%d %H:%M:%S+0000 \")\n",
    "topic_narrow['date']=topic_narrow['date_time'].dt.normalize()\n",
    "topic_narrow['month']=topic_narrow['date_time'].dt.month\n",
    "topic_narrow['week']=topic_narrow['date_time'].dt.week\n",
    "topic_narrow['year']=topic_narrow['date_time'].dt.year\n",
    "#topic['hour']=topic['date_time'].dt.hour\n",
    "#topic['minute']=topic['date_time'].dt.minute\n",
    "topic_narrow['second']=topic_narrow['date_time'].dt.second\n",
    "topic_narrow['year_week']=topic_narrow['year']*100+topic_narrow['week']\n",
    "\n",
    "topic_narrow['hashtags']=topic_narrow['text'].astype(str).apply(get_hashtags)\n",
    "\n",
    "topic_narrow['days_from_birth']=(topic_narrow['date']-topic_narrow['birth_date'])/np.timedelta64(1, 'D')\n",
    "topic_narrow['follower_per_update']=topic_narrow['user.followers_count'].astype('float')/topic_narrow['user.statuses_count'].astype('float')\n",
    "topic_narrow['follower_per_friend']=topic_narrow['user.followers_count'].astype('float')/topic_narrow['user.friends_count'].astype('float')\n",
    "topic_narrow['tweets_per_day']=topic_narrow['user.statuses_count'].astype('float')/topic_narrow['days_from_birth']\n",
    "topic_narrow['short_content']=topic_narrow['text'].str.replace(r'http.*', 'http')\n",
    "topic_narrow['is_verified']=topic_narrow['user.verified']==True\n",
    "topic_narrow['word_count'] = topic_narrow['text'].str.split().str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify original tweets that repeat vertbatim and how many of those tweets are verified (Consider near-verbatim)\n",
    "\n",
    "repeats=topic_narrow[topic_narrow['retweeted_status.id'].isnull()][['is_verified','id_str','short_content']].groupby('short_content').agg({'id_str':'count','is_verified':'sum'}).reset_index().rename(columns={'id_str':'repeats_count','is_verified':'verified_repeat'})\n",
    "topic_narrow=topic_narrow.merge(repeats, how='left', on='short_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_narrow['is_verified'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Key Tweet/Account Label Definitions:\n",
    "# Deleted -- appears in SS but not in Twitter API\n",
    "# Verified \n",
    "# Egg -- Default Profile Image\n",
    "# First -- Tweet occurs in first second of the minute; indicator of automation.\n",
    "# Flood -- Tweet is from a non-verified user and is an exact duplicate of another tweet and that \n",
    "#          tweet is not duplicated by a verified user\n",
    "# Bot -- Account with more than 320 tweets per day\n",
    "# Baby -- Account with 100 tweets or fewer\n",
    "# Train -- Account with follower count within 10% of  following count, at least 1000 friends, and at least 100 tweets\n",
    "# Troll -- Non-verified user with at least 1000 friends 100 tweets, and \"too many\" followers per tweet >0.75 . \n",
    "\n",
    "\n",
    "topic_narrow['deleted']=topic_narrow['id_str'].isnull()\n",
    "topic_narrow['verified']=(topic_narrow['user.verified']==True)\n",
    "topic_narrow['egg']=(topic_narrow['user.default_profile_image']==True)\n",
    "topic_narrow['first']=((topic_narrow['user.verified']==False)\n",
    "                &(topic_narrow['second']==1)\n",
    "               )\n",
    "topic_narrow['flood']=((topic_narrow['user.verified']==False)\n",
    "                &(topic_narrow['POST_TYPE']!='RETWEET')\n",
    "                &(topic_narrow['repeats_count']>2)\n",
    "                 &(topic_narrow['verified_repeat']==0)\n",
    "                     &(topic_narrow['word_count']>2)\n",
    "               )\n",
    "topic_narrow['bot']=((topic_narrow['user.verified']==False)\n",
    "                &(topic_narrow['tweets_per_day']>320)\n",
    "               )\n",
    "topic_narrow['baby']=((topic_narrow['user.verified']==False)\n",
    "            &(topic_narrow['user.statuses_count'].astype('float')<=100) \n",
    "            )\n",
    "topic_narrow['odd_client']=~((topic_narrow['source'].str.contains('<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',na=False))\n",
    "            |(topic_narrow['source'].str.contains('<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>',na=False)) \n",
    "            |(topic_narrow['source'].str.contains('<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',na=False))\n",
    "            |(topic_narrow['source'].str.contains('<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>',na=False))\n",
    "            )\n",
    "\n",
    "topic_narrow['train']=((topic_narrow['user.verified']==False) \n",
    "            &(topic_narrow['user.statuses_count'].astype('float')>100) \n",
    "            &(topic_narrow['user.friends_count'].astype('float')>1000) \n",
    "            &(topic_narrow['follower_per_friend']>.9)\n",
    "             &(topic_narrow['follower_per_friend']<1.1)\n",
    "            )\n",
    "topic_narrow['troll']=((topic_narrow['user.verified']==False)\n",
    "            &(topic_narrow['user.statuses_count'].astype('float')>100) \n",
    "            &(topic_narrow['user.friends_count'].astype('float')>1000) \n",
    "            &(topic_narrow['user.statuses_count'].astype('float')<10000) \n",
    "            &(topic_narrow['follower_per_update']>.75) \n",
    "            )\n",
    "\n",
    "#topic_narrow['punct']=((topic_narrow['user.verified']==False)\n",
    "#            &(topic_narrow['text'].str.contains(u\"\\u2019\")) \n",
    "#                      )\n",
    "    \n",
    "topic_narrow['any']=topic_narrow['troll']|topic_narrow['train']|topic_narrow['baby']|topic_narrow['bot']|topic_narrow['flood']|topic_narrow['first']|topic_narrow['deleted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=['troll' ,'train' ,'baby' ,'bot' , 'flood','first','verified','egg','odd_client']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define dictionary for groupby() aggregation in conversations approach, below.\n",
    "\n",
    "group_dict={'id_str':'count'}\n",
    "group_dict['date']='first'\n",
    "for type in label_list:\n",
    "    group_dict[type]='mean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pre-Specified Conversations Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a set of conversations by providing a list of terms, where any tweet containing the term is in the convo\n",
    "conversations=['trust','mail','home','vote']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A In the Cross-section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize share of tweets in each convo, and overall, with each label\n",
    "\n",
    "means=topic_narrow[label_list].mean()\n",
    "plt.figure(figsize=(20,10))\n",
    "barwidth=.8/len(conversations)\n",
    "\n",
    "plt.bar([x-.4-barwidth for x in range(len(label_list))], means, width=barwidth, align='center', alpha=0.5, label='Overall')\n",
    "\n",
    "i=0\n",
    "for convo in conversations:\n",
    "    print(convo)\n",
    "    print(topic_narrow[topic_narrow['text'].str.contains(convo, na=False, case=False)].shape)\n",
    "    means=topic_narrow[topic_narrow['text'].str.contains(convo, na=False, case=False)][label_list].mean()\n",
    "    \n",
    "\n",
    "    plt.bar([x-.4 + i*barwidth for x in range(len(label_list))], means, width=barwidth, align='center', alpha=0.5, label=convo)\n",
    "    plt.xticks([x for x in range(len(label_list))], label_list,  fontsize=20)\n",
    "    i=i+1\n",
    "\n",
    "    \n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B Over Time Daily and/or Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut conversations into days, and calculate label shares for each convo x day, as well as counts.\n",
    "\n",
    "topic_narrow['date']=topic_narrow['date_time'].dt.date\n",
    "daily_stats=topic_narrow[fulltext_narrow][label_list+['date','id_str']].groupby('date').agg(group_dict).rename(columns={'id_str':'tweet_count'})\n",
    "daily_stats['convo']='Topic'\n",
    "for convo in conversations:\n",
    "    convo_screen=topic_narrow['text'].str.contains(convo, na=False, case=False)\n",
    "    convo_stats=topic_narrow[fulltext_narrow&convo_screen][label_list+['date','id_str']].groupby('date').agg(group_dict).rename(columns={'id_str':'tweet_count'})\n",
    "    convo_stats['convo']=convo\n",
    "    daily_stats=daily_stats.append(convo_stats)\n",
    "\n",
    "daily_stats['log_count']=np.log(daily_stats['tweet_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly version\n",
    "\n",
    "weekly_stats=topic_narrow[fulltext_narrow][label_list+['year_week','id_str','date']].groupby('year_week').agg(group_dict).rename(columns={'id_str':'tweet_count'})\n",
    "weekly_stats['convo']='Overall'\n",
    "for convo in conversations:\n",
    "    convo_screen=topic_narrow['text'].str.contains(convo, na=False, case=False)\n",
    "    convo_stats=topic_narrow[fulltext_narrow&convo_screen][label_list+['year_week','date','id_str']].groupby('year_week').agg(group_dict).rename(columns={'id_str':'tweet_count'})\n",
    "    convo_stats['convo']=convo\n",
    "    weekly_stats=weekly_stats.append(convo_stats)\n",
    "\n",
    "weekly_stats['log_count']=np.log(weekly_stats['tweet_count'])\n",
    "weekly_stats.index=weekly_stats['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For axis ticks on timelines\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "days = mdates.DayLocator()  # every month\n",
    "hours = mdates.HourLocator()  # every month\n",
    "yearsFmt = mdates.DateFormatter('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daily Graphs, one frame for each account type and an overall tweet (log) count\n",
    "\n",
    "min_tweet=10\n",
    "fig, ax=plt.subplots(len(label_list)+1,1,figsize=(20,20))\n",
    "\n",
    "convo_list=daily_stats['convo'].drop_duplicates().sort_values()\n",
    "for i in range(0,len(label_list)):\n",
    "        for convo in convo_list:\n",
    "            ax[i].plot(daily_stats[(daily_stats['convo']==convo)&(daily_stats['tweet_count']>min_tweet)][label_list[i]].sort_index(), alpha=.7, label=convo)\n",
    "        ax[i].set_title(\"Share \"+label_list[i], fontsize=12)\n",
    "        ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "        #ax[i].legend()\n",
    "        #Adjust these if there are many days\n",
    "        ax[i].xaxis.set_minor_locator(days)\n",
    "        ax[i].xaxis.set_major_locator(months)\n",
    "        ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "for convo in convo_list:\n",
    "    ax[len(label_list)].plot(daily_stats[daily_stats['convo']==convo]['log_count'].sort_index(), alpha=.7, label=convo)\n",
    "\n",
    "###Adjust these if there are many days###    \n",
    "ax[len(label_list)].xaxis.set_minor_locator(days)\n",
    "ax[len(label_list)].xaxis.set_major_locator(months)\n",
    "ax[len(label_list)].xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "\n",
    "ax[len(label_list)].tick_params(axis='both', which='major', labelsize=10)\n",
    "ax[len(label_list)].set_title('Log(Tweet Count)', fontsize=12)\n",
    "ax[0].legend()\n",
    "\n",
    "fig.tight_layout(pad=.5)\n",
    "\n",
    "#ax[i].set_ylim([0,.2])\n",
    "#plt.savefig('stat_timeline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekly Version\n",
    "min_tweet=30\n",
    "\n",
    "fig, ax=plt.subplots(len(label_list)+1,1,figsize=(20,20))\n",
    "\n",
    "convo_list=weekly_stats['convo'].drop_duplicates().sort_values()\n",
    "for i in range(0,len(label_list)):\n",
    "        for convo in convo_list:\n",
    "            ax[i].plot(weekly_stats[(weekly_stats['convo']==convo)&(weekly_stats['tweet_count']>min_tweet)][label_list[i]].sort_index(), alpha=.7, label=convo)\n",
    "        ax[i].set_title(\"Share \"+label_list[i], fontsize=12)\n",
    "       #ax[i].legend()\n",
    "        ax[i].xaxis.set_major_locator(months)\n",
    "        #ax[i].xaxis.set_minor_locator(months)\n",
    "        ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "for convo in convo_list:\n",
    "    ax[len(label_list)].plot(weekly_stats[weekly_stats['convo']==convo]['log_count'].sort_index(), alpha=.7, label=convo)\n",
    "ax[len(label_list)].xaxis.set_major_locator(months)\n",
    "#ax[len(label_list)].xaxis.set_minor_locator(months)\n",
    "ax[len(label_list)].tick_params(axis='both', which='major', labelsize=10)\n",
    "ax[len(label_list)].xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "ax[len(label_list)].set_title('Log(Tweet Count)', fontsize=12)\n",
    "\n",
    "fig.tight_layout(pad=.5)\n",
    "\n",
    "ax[0].legend()\n",
    "#ax[i].set_ylim([0,.2])\n",
    "#plt.savefig('stat_timeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hashtag Detection Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.A Gather Top Hashtags in Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package for counting\n",
    "import collections\n",
    "\n",
    "\n",
    "# Define the minimum times a hashtag needs to appear in the overall topic in order or on the day to include it in the analysis\n",
    "\n",
    "min_size_week=50\n",
    "min_size_day=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularize capitalization\n",
    "\n",
    "topic_narrow['lower_text']=topic_narrow['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation and gather hashtags as list\n",
    "\n",
    "topic_narrow['lower_hashtags']=topic_narrow['lower_text'].astype(str).replace('\\,|\\.', '',regex=True).apply(get_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate list of hashtags, for counting\n",
    "\n",
    "hashtags = [x for l in topic_narrow[topic_narrow['lang']=='en'].lower_hashtags.values for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counter object\n",
    "\n",
    "counter = collections.Counter(hashtags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top-X most common hashtags, overall, with counts\n",
    "testtemp=counter.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list just the hashtags, dropping a hashtag that matches the stem that defined the topic, if appropriate\n",
    "\n",
    "long_convo=[i[0] for i in testtemp if ('#'+stem.lower()!=(str(i[0])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.B Weekly Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By week, by top-200 (overall) hashtag, calculate share of tweets with each label\n",
    "\n",
    "weekly_detection=pd.DataFrame()\n",
    "for convo in long_convo:\n",
    "    convo_screen=topic_narrow['lower_hashtags'].apply(lambda x: convo in x )\n",
    "    convo_stats=topic_narrow[fulltext_narrow&convo_screen][label_list+['year_week','date','id_str']].groupby('year_week').agg(group_dict).rename(columns={'id_str':'tweet_count'})\n",
    "    convo_stats['convo']=convo\n",
    "    weekly_detection=weekly_detection.append(convo_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of hashtag x weeks combos that clear the minimum requirement set by min_size_week\n",
    "\n",
    "weekly_detection[weekly_detection['tweet_count']>min_size_week].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each week, calculate the mean/sd of shares of each label across the qualifying hashtags, as well as for the whole set of weeks\n",
    "\n",
    "weekly_means=weekly_detection[weekly_detection['tweet_count']>min_size_week].groupby('year_week').mean()\n",
    "weekly_std=weekly_detection[weekly_detection['tweet_count']>min_size_week].groupby('year_week').std()\n",
    "overall_std=weekly_detection[weekly_detection['tweet_count']>min_size_week].std()\n",
    "overall_mean=weekly_detection[weekly_detection['tweet_count']>min_size_week].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcuate how many standard-deviations away from the overall mean each week x label share is.\n",
    "\n",
    "for type in label_list:\n",
    "    weekly_detection[type+'norm']=(weekly_detection[type]-overall_mean[type])/overall_std[type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each label, output all the qualifying hashtags x week shares that are more than 2 sd above the mean.\n",
    "\n",
    "for type in label_list:\n",
    "    print(type)\n",
    "    print(weekly_detection[(weekly_detection['tweet_count']>min_size_week)&(weekly_detection[type+'norm']>2)].sort_index()[['convo',type,'tweet_count']])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overall_mean)\n",
    "print(overall_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting means and sd by week\n",
    "weekly_outliers=weekly_means.merge(weekly_std, on='year_week',how='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the outlier thresholds by week and label\n",
    "for type in label_list:\n",
    "    weekly_outliers[type+\"_plus_2sd\"]=weekly_outliers[type+'_x']+2*weekly_outliers[type+'_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building panel at the convo x week level, for qualifying convos\n",
    "weekly_det2=weekly_detection[weekly_detection['tweet_count']>min_size_week].merge(weekly_outliers, on='year_week',how='left').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_det2.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Output quick text file of outliers [stem].txt\n",
    "\n",
    "file1 = open(stem+\".txt\",\"w\")#write mode \n",
    "for week in weekly_det2['year_week'].drop_duplicates().sort_values():\n",
    "    file1.write('Week: '+str(week)+'\\n') \n",
    "    print('Week: '+str(week))\n",
    "    for type in label_list:\n",
    "        hashlist=weekly_det2[(weekly_det2[type]>weekly_det2[type+'_plus_2sd'])&(weekly_det2['year_week']==week)]['convo'].to_list()\n",
    "        if len(hashlist)>0:\n",
    "            file1.write(type+': '+str(hashlist)+'\\n')\n",
    "            print(type+': '+str(hashlist))\n",
    "\n",
    "file1.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.C Daily Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By day, by top-200 (overall) hashtag, calculate share of tweets with each label\n",
    "\n",
    "\n",
    "daily_detection=pd.DataFrame()\n",
    "for convo in long_convo:\n",
    "    convo_screen=topic_narrow['lower_hashtags'].apply(lambda x: convo in x )\n",
    "    convo_stats=topic_narrow[fulltext_narrow&convo_screen][label_list+['date','id_str']].groupby('date').agg(group_dict).rename(columns={'id_str':'tweet_count'}).reset_index(drop=True)\n",
    "    convo_stats['convo']=convo\n",
    "    daily_detection=daily_detection.append(convo_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each day, calculate the mean/sd of shares of each label across the qualifying hashtags, as well as for the whole set of days\n",
    "\n",
    "\n",
    "daily_means=daily_detection[daily_detection['tweet_count']>min_size_day].groupby('date').mean()\n",
    "daily_std=daily_detection[daily_detection['tweet_count']>min_size_day].groupby('date').std()\n",
    "d_overall_std=daily_detection[daily_detection['tweet_count']>min_size_day].std()\n",
    "d_overall_mean=daily_detection[daily_detection['tweet_count']>min_size_day].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcuate how many standard-deviations away from the overall mean each day  x label share is.\n",
    "\n",
    "\n",
    "for type in label_list:\n",
    "    daily_detection[type+'norm']=(daily_detection[type]-d_overall_mean[type])/d_overall_std[type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_detection[daily_detection['tweet_count']>min_size_day].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each label, output all the qualifying hashtags x day shares that are more than 2 sd above the mean.\n",
    "\n",
    "for type in label_list:\n",
    "    print(type)\n",
    "    print(daily_detection[(daily_detection['tweet_count']>min_size_day)&(daily_detection[type+'norm']>2)].sort_index()[['convo',type,'tweet_count']])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_overall_mean)\n",
    "print(d_overall_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put together daily means and std. dev.\n",
    "daily_outliers=daily_means.merge(daily_std, on='date',how='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define what qualifies as a 2 Std.Dev. daily outlier for each label\n",
    "for type in label_list:\n",
    "    daily_outliers[type+\"_plus_2sd\"]=daily_outliers[type+'_x']+2*daily_outliers[type+'_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build day x convo panel for qualifying hashtags\n",
    "\n",
    "daily_det2=daily_detection[daily_detection['tweet_count']>min_size_day].merge(daily_outliers, on='date',how='left').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output quick text file of outliers [stem]_d.txt\n",
    "\n",
    "\n",
    "file1 = open(stem+\"_d.txt\",\"w\")#write mode \n",
    "for day in daily_det2['date'].drop_duplicates().sort_values():\n",
    "    file1.write('Date: '+str(day)+'\\n') \n",
    "    print('Date: '+str(day))\n",
    "    for type in label_list:\n",
    "        hashlist=daily_det2[(daily_det2[type]>daily_det2[type+'_plus_2sd'])&(daily_det2['date']==day)]['convo'].to_list()\n",
    "        if len(hashlist)>0:\n",
    "            file1.write(type+': '+str(hashlist)+'\\n')\n",
    "            print(type+': '+str(hashlist))\n",
    "\n",
    "file1.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Unsupervised Conversation Detection with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_narrow['week'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.A Narrow Dataset for NLP-- Time and Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP really needs a common languange and consistent meanings. Zoom in on specific langauge/time. This depends on Twitter's\n",
    "#Language guess. It's not great.\n",
    "\n",
    "dataset=topic_narrow[(topic_narrow['lang']=='en')].drop_duplicates('text').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_narrow['lang'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.B Clean, Stem, and Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    tweet = re.sub('\\n', ' ',tweet) # remove [links]\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_text']=dataset['text'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_text']=dataset['clean_text'].apply(remove_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem words from text to combine variants of a word into common stem, using NLTK's Snowball Stemmer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "dataset['stemmed'] = dataset.clean_text.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "dataset.stemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.stemmed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the CountVectorizer/TfidfTransformer approach... to build subset of words that really stand out to use in a bag\n",
    "# Options:\n",
    "# min_df -- min share of tweets a word needs to show up in to be included\n",
    "# max_df -- min share of tweets a word needs to show up in to be included\n",
    "# ngram_range(min,max) -- range of n-gram lengths to include\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"https\",'amp', stem])\n",
    "\n",
    "cvec = CountVectorizer(stop_words=my_stop_words, min_df=.001, max_df=.75, ngram_range=(1,2))\n",
    "cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_counts = cvec.fit_transform(dataset.stemmed)\n",
    "\n",
    "print( 'sparse matrix shape:', cvec_counts.shape)\n",
    "print ('nonzero count:', cvec_counts.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * cvec_counts.nnz / (cvec_counts.shape[0] * cvec_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top words\n",
    "\n",
    "occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, weight the words for prediction where we especially focus on the words that are 'odd', in terms of showing up rarely in the overall corpus \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(cvec_counts)\n",
    "transformed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of features\n",
    "\n",
    "feature_names = cvec.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.C K-Means Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider various cluster counts, and see how well clustering works when we do mini-batches at each K\n",
    "# Then, display how well the k-means fit (in terms of SSE), for each choice of k\n",
    "k_max=50\n",
    "\n",
    "\n",
    "K = range(1,k_max)\n",
    "SSE = []\n",
    "dSSE=[]\n",
    "for k in K:\n",
    "    kmeans = MiniBatchKMeans(n_clusters = k,batch_size = 1000)\n",
    "    kmeans.fit(transformed_weights)\n",
    "    SSE.append(kmeans.inertia_)\n",
    "    if k>2:\n",
    "        dSSE.append(SSE[k-1]-SSE[k-2])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(K,SSE,'bx-')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('cluster numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Easier to read in differences. Looking for a point where the SSE improvement stops getting better very fast.\n",
    "\n",
    "plt.plot(range(0,k_max-3),dSSE[0:k_max],'bx-')\n",
    "plt.title('d Elbow Method')\n",
    "plt.xlabel('cluster numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the k we're going to use\n",
    "\n",
    "k=10\n",
    "kmeans = MiniBatchKMeans(n_clusters = k, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Present the words that are closest to the center of each cluster.\n",
    "\n",
    "kmeans.fit(transformed_weights)\n",
    "centers = kmeans.cluster_centers_.argsort()[:,::-1]\n",
    "\n",
    "for i in range(0,k):\n",
    "    word_list=[]\n",
    "    print(\"cluster%d:\"% i)\n",
    "    for j in centers[i,:15]:\n",
    "        word_list.append(feature_names[j])\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate predicted cluster for each tweet in dataset\n",
    "\n",
    "clusters_predict=kmeans.predict(transformed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['k_cluster']=clusters_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['k_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.D Latent Dirichlet Allocation to Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#Define LDA model:\n",
    "# Parameters:\n",
    "# number_of_topics -- How many clusters to create (we currently use whatever looked best in the k-means)\n",
    "#\n",
    "\n",
    "number_of_topics = k\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the TF-IDF transformed BOW model, using LDA algorithm with default parameters \n",
    "#(https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "\n",
    "model.fit(transformed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display top features in each topic\n",
    "\n",
    "def display_topics(model, f_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(f_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually do the display\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(model, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document â€” Topic Matrix\n",
    "lda_output = model.transform(transformed_weights)\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(dataset['text']))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.reset_index(drop=True).join(df_document_topic['dominant_topic'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['dominant_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.E Visualize/Analyze k-Mean/LDA Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge back\n",
    "\n",
    "merged_topic=topic_narrow.merge(dataset[['text','k_cluster','dominant_topic']], how='left',on='text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_topic[['text','k_cluster','dominant_topic']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Mean Cross-Section Visualization\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "barwidth=.9/k\n",
    "for i in range(0,k):\n",
    "\n",
    "    means=merged_topic[merged_topic['k_cluster']==i][label_list].mean()\n",
    "    \n",
    "\n",
    "    plt.bar([x-.5 + i*barwidth for x in range(len(label_list))], means, width=barwidth, align='center', alpha=0.5)\n",
    "    plt.xticks([x for x in range(len(label_list))], label_list,  fontsize=20)\n",
    "    i=i+1\n",
    "\n",
    "means=merged_topic[label_list].mean()\n",
    "    \n",
    "#plt.bar([x-.5 for x in range(len(label_list))], means, width=barwidth, align='center', alpha=0.5)\n",
    "plt.legend(  range(0,k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cluster_means=merged_topic[label_list+['k_cluster','id_str']].groupby('k_cluster').agg({'troll':'mean' ,'train':'mean' ,'baby':'mean' ,'bot':'mean' , 'flood':'mean','first':'mean','verified':'mean','egg':'mean','id_str':'count'}).rename(columns={'id_str':'tweet_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cluster_means.sort_values('flood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Cross-Section Visualization\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "barwidth=.9/k\n",
    "for i in range(0,k):\n",
    "\n",
    "    means=merged_topic[merged_topic['dominant_topic']==i][label_list].mean()\n",
    "    \n",
    "\n",
    "    plt.bar([x-.5 + i*barwidth for x in range(len(label_list))], means, width=barwidth, align='center', alpha=0.5)\n",
    "    plt.xticks([x for x in range(len(label_list))], label_list,  fontsize=20)\n",
    "    i=i+1\n",
    "\n",
    "means=dataset[label_list].mean()\n",
    "    \n",
    "#plt.bar([x-.5 for x in range(len(label_list))], means, width=barwidth, align='center', alpha=0.5)\n",
    "plt.legend(  range(0,k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_means=merged_topic[label_list+['dominant_topic','id_str']].groupby('dominant_topic').agg({'troll':'mean' ,'train':'mean' ,'baby':'mean' ,'bot':'mean' , 'flood':'mean','first':'mean','verified':'mean','egg':'mean','id_str':'count'}).rename(columns={'id_str':'tweet_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_means.sort_values('flood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  Hunt Graphs for Accounts from Suspicious Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Wordclouds\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "stopwords= set(STOPWORDS)\n",
    "\n",
    "new_stopwords=stopwords.union({'RT\\'','Follow\\'', 'https','co','amp'})\n",
    "wc = WordCloud(width=800, \n",
    "               height=400,\n",
    "               max_font_size=100, \n",
    "               max_words=40,\n",
    "               stopwords=new_stopwords,\n",
    "               background_color='black',\n",
    "               normalize_plurals= True,\n",
    "              collocations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.A Specify target by terms and time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options for indicating subset of topic to target\n",
    "\n",
    "week_target=merged_topic['week']==33.0\n",
    "day_target=merged_topic['date']=='2020-06-05'\n",
    "text_target=(merged_topic['text'].str.contains('mail',na=False, case=False))|(merged_topic['text'].str.contains('absentee',na=False, case=False))\n",
    "cluster_target=(merged_topic['k_cluster']==16)\n",
    "type_target=(merged_topic['any'])&~(merged_topic['verified'])\n",
    "\n",
    "#aggregation of conditions to specify final target as mask of topic_narrow\n",
    "target=text_target&type_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick glance at targeted tweets\n",
    "\n",
    "#print(topic_narrow[target].shape)\n",
    "#topic_narrow[target][['user.screen_name','text']].head()\n",
    "\n",
    "merged_topic[target][['text']].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.B Collect Info on Targeted Accounts and Make Hunt Graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect screen_names of users in target (Maybe.. change this to userid, since its better for tracking the account)\n",
    "\n",
    "#timing_names=topic_narrow[target].drop_duplicates(subset=['user.screen_name'])['user.screen_name'].reset_index(drop=True)\n",
    "timing_names=merged_topic[target&(merged_topic['user.followers_count']>100)].drop_duplicates(subset=['user.screen_name'])['user.screen_name'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_names= pd.Series(list(set(timing_names_2) & set(timing_names_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(timing_names_1.shape)\n",
    "#print(timing_names_2.shape)\n",
    "print(timing_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather 200 most recent tweets from all accounts in target\n",
    "\n",
    "timing_tweets=[]\n",
    "for timing_name in timing_names:\n",
    "    try: \n",
    "            timing_tweets.extend(twitter_api.user_timeline(screen_name=timing_name, count=200))\n",
    "    except tweepy.TweepError:\n",
    "            print('Missing: '+timing_name)\n",
    "    \n",
    "json_data2 = [r._json for r in timing_tweets]\n",
    "timing_tweets_pd = pd.json_normalize(json_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate stats for visualization\n",
    "\n",
    "timing_tweets_pd['date_time']=pd.to_datetime(timing_tweets_pd['created_at'], format=\"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "timing_tweets_pd['date']=timing_tweets_pd['date_time'].dt.normalize()\n",
    "#timing_tweets_pd['month']=timing_tweets_pd['date_time'].dt.month\n",
    "#timing_tweets_pd['week']=timing_tweets_pd['date_time'].dt.week\n",
    "#timing_tweets_pd['year']=timing_tweets_pd['date_time'].dt.year\n",
    "timing_tweets_pd['hour']=timing_tweets_pd['date_time'].dt.hour\n",
    "timing_tweets_pd['hashtags']=timing_tweets_pd['text'].apply(get_hashtags)\n",
    "timing_tweets_pd['birthday']=pd.to_datetime(timing_tweets_pd['user.created_at'], format=\"%a %b %d %H:%M:%S +0000 %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#collapse back down to list of accounts for interation/account-level info presentation\n",
    "\n",
    "timing_tweets_list=timing_tweets_pd.groupby('user.screen_name')[['date','birthday','user.followers_count','user.friends_count']].agg(first_date=('date','min'), last_date=('date','max'),birthday=('birthday','first'),following_count=('user.friends_count','first'),follower_count=('user.followers_count','first'),tweet_count=('date','count')).reset_index()\n",
    "timing_tweets_list.sort_values('birthday', inplace=True)\n",
    "timing_tweets_list['days']=(timing_tweets_list['last_date']-timing_tweets_list['first_date'])/np.timedelta64(1, 'D')+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution across hour of day and client, present hashtags, and some account state\n",
    "# 100 Accounts per file\n",
    "# I would love to make this a pdf with clickable links to account, but I can't figure out how \n",
    "#\n",
    "\n",
    "hours=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "\n",
    "for j in range(0,len(timing_tweets_list['user.screen_name']),100):\n",
    "    account_list=timing_tweets_list['user.screen_name'][j:j+100].reset_index(drop=True)\n",
    "    days=timing_tweets_list['days'][j:j+100].reset_index(drop=True)\n",
    "    tweets=timing_tweets_list['tweet_count'][j:j+100].reset_index(drop=True)\n",
    "    fols=timing_tweets_list['follower_count'][j:j+100].reset_index(drop=True)\n",
    "    \n",
    "    #description_list=timing_tweets_pd[['user.screen_name','user.description']].drop_duplicates()['user.description'][j:j+100].reset_index(drop=True)\n",
    "    fig, ax=plt.subplots(len(account_list),2,figsize=(10,(4*len(account_list))))\n",
    "    \n",
    "    for i in range(0,len(account_list)):\n",
    "        for type in (timing_tweets_pd[timing_tweets_pd['user.screen_name']==account_list[i]]['source'].drop_duplicates()):\n",
    "            if type=='<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>':\n",
    "                ax[i,0].hist(timing_tweets_pd[(timing_tweets_pd['user.screen_name']==account_list[i])&(timing_tweets_pd['source']==type)]['hour'], hours ,alpha=.7, label=type, color='red')\n",
    "            elif type=='<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>': \n",
    "                ax[i,0].hist(timing_tweets_pd[(timing_tweets_pd['user.screen_name']==account_list[i])&(timing_tweets_pd['source']==type)]['hour'], hours ,alpha=.7, label=type, color='skyblue')\n",
    "            elif type=='<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>': \n",
    "                ax[i,0].hist(timing_tweets_pd[(timing_tweets_pd['user.screen_name']==account_list[i])&(timing_tweets_pd['source']==type)]['hour'], hours ,alpha=.7, label=type, color='orange')\n",
    "            elif type=='<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>': \n",
    "                ax[i,0].hist(timing_tweets_pd[(timing_tweets_pd['user.screen_name']==account_list[i])&(timing_tweets_pd['source']==type)]['hour'], hours ,alpha=.7, label=type, color='lightgreen')\n",
    "            elif type=='<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>': \n",
    "                ax[i,0].hist(timing_tweets_pd[(timing_tweets_pd['user.screen_name']==account_list[i])&(timing_tweets_pd['source']==type)]['hour'], hours ,alpha=.7, label=type, color='yellow')\n",
    "            else:\n",
    "                ax[i,0].hist(timing_tweets_pd[(timing_tweets_pd['user.screen_name']==account_list[i])&(timing_tweets_pd['source']==type)]['hour'], hours ,alpha=.7, label=type, color='grey')\n",
    "            ax[i,0].legend()\n",
    "            ax[i,0].set_title(account_list[i], fontsize=12)\n",
    "           # plt.title(r\"\\href{http://www.google.com}{This title links to google}\", color='gray')\n",
    "\n",
    "        Hashtag_Combined = \" \".join(timing_tweets_pd[(timing_tweets_pd['user.screen_name']==account_list[i])&(timing_tweets_pd['hashtags'].astype(str)!='[]')]['hashtags'].astype(str))\n",
    "        \n",
    "        if Hashtag_Combined!='':\n",
    "            try:\n",
    "                wc.generate(Hashtag_Combined)\n",
    "                ax[i,1].imshow(wc, interpolation=\"bilinear\")\n",
    "                ax[i,1].axis(\"off\")\n",
    "            except ValueError:\n",
    "                print('No Hash')\n",
    "        ax[i,1].set_title('D: '+str(days[i])+\" T: \"+str(tweets[i])+\" F: \"+str(fols[i]), fontsize=12)\n",
    "      \n",
    "        \n",
    "        \n",
    "    plt.savefig(stem+'_mail_baseline_'+str(j)+'.png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 2019.10)",
   "language": "python",
   "name": "anaconda3-2019.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
