{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# vim:fenc=utf-8\n",
    "\"\"\"\n",
    "Twitter API Streaming\n",
    "By David White\n",
    "\"\"\"\n",
    "\n",
    "__version__ = '0.1.0'\n",
    "\n",
    "required = ['requests', 'pandas', 'os', 'json', 'time']\n",
    "\n",
    "# Adds the token to the local environment\n",
    "import os\n",
    "os.environ['BEARER_TOKEN'] = 'YOUR BEARER TOKEN'\n",
    "\n",
    "import pandas as pd\n",
    "# Create a CSV file on the local system to store the data\n",
    "store = pd.DataFrame(columns = ['conversation_id','created_at','author_id','lang','text','name','username','geo.place_id'])\n",
    "store.info()\n",
    "store.to_csv('store.csv')\n",
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "#Uncomment to verify the bearer token\n",
    "#print(bearer_token)\n",
    "\n",
    "def connect_to_twitter():\n",
    "    bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "headers = connect_to_twitter()\n",
    "\n",
    "while(True):\n",
    "    import time\n",
    "    #print('I am Alive!'+'/n')\n",
    "\n",
    "    def make_request(headers):\n",
    "        import requests\n",
    "        import json\n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "        query_params = {'query': 'xinjiang',\n",
    "                        'max_results': 10,\n",
    "                        'expansions': 'author_id,geo.place_id',\n",
    "                        'tweet.fields': 'created_at,lang'\n",
    "                        #'user.fields': 'username'\n",
    "                        }\n",
    "        return requests.request(\"GET\", url, params=query_params,    headers=headers).json()\n",
    "    response = make_request(headers)\n",
    "    #print(response)\n",
    "\n",
    "    tweet_data = response['data']\n",
    "    tweet_data = pd.json_normalize(tweet_data)\n",
    "    tweet_data.rename(columns = {'id':'conversation_id'}, inplace = True)\n",
    "    tweet_data.rename(columns = {'id':'conversation_id'}, inplace = True)\n",
    "    if 'geo.place_id' in tweet_data.columns:\n",
    "            print(tweet_data)\n",
    "            tweet_data_reorder = tweet_data.reindex(columns=['conversation_id','created_at','author_id','lang','text','geo.place_id'])\n",
    "            tweet_data_reorder.info()\n",
    "    else:\n",
    "            tweet_data.loc[:,'geo.place_id'] = ''\n",
    "            #print(tweet_data_reorder)\n",
    "            tweet_data_reorder = tweet_data.reindex(columns=['conversation_id','created_at','author_id','lang','text','geo.place_id'])\n",
    "            tweet_data_reorder.info()\n",
    "    #print(tweet_data)\n",
    "    #tweet_data['conversation_id'] = tweet_data['id']\n",
    " \n",
    "    # \n",
    "\n",
    "    tweet_user = response['includes']\n",
    "    tweet_user = pd.json_normalize(tweet_user,record_path=['users'])\n",
    "    tweet_user.rename(columns = {'id':'author_id'}, inplace = True)\n",
    "    tweet_user.reindex(columns=['author_id','name','username'])\n",
    "    #tweet_user['author_id'] = tweet_user['id']\n",
    "    \n",
    "    tweet_user.info()\n",
    "    #print(tweet_user)\n",
    "\n",
    "    result = pd.merge(tweet_data_reorder, tweet_user.rename(columns={'id':'author_id'}), on='author_id',  how='left')\n",
    "    result_reorder = result[['conversation_id','created_at','author_id','lang','text','name','username','geo.place_id']]\n",
    "\n",
    "    result_reorder.info()\n",
    "\n",
    "    result_reorder.to_csv('store.csv', mode='a', index=True, header=False)\n",
    "\n",
    "    time.sleep(90)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
